{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read this in conjuction with my post on medium\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need whole bunch of libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619165, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load file from modified raw data set\n",
    "df=pd.read_pickle(\"uniform.pkl\")\n",
    "# Check the size of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have created a file prep2 containing functions to be used in multiprocessing code below\n",
    "# This is because of a bug in Jupyter that cause code to be stuck in an infinite loop, if these functions are build here.\n",
    "import prep2\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "%config IPCompleter.greedy=True\n",
    "# This part was needed as any changes to the function in prep2.py is not reflected without this.\n",
    "import importlib\n",
    "importlib.reload(prep2)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2880115_1.0\n",
       "1    1300387_1.0\n",
       "2    4487515_1.0\n",
       "3    6588495_1.0\n",
       "4    3653311_1.0\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use this a document Id for Doc2vec tagging\n",
    "df['token']=  df[\"index\"].map(str)+\"_\" + df[\"overall\"].map(str)\n",
    "df['token'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful references on this:\n",
    "# https://praveenbezawada.com/2018/01/25/document-similarity-using-gensim-dec2vec/\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocessing and tokenizing the reviews.\n",
    "docs=df[['reviewText','token']]\n",
    "docs['TokenizedText']=prep2.parallelize(docs['reviewText'], prep2.word_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Doc2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized text are list in each cell of dataframe. \n",
    "# This is doc2vec software needs to convert elements in a column in a list and attach a tag to it\n",
    "# This was borrowed from here: https://praveenbezawada.com/2018/01/25/document-similarity-using-gensim-dec2vec/\n",
    "\n",
    "# Tagging the documents\n",
    "class TaggedDocumentIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc, tags=[self.labels_list[idx]])\n",
    " \n",
    "docLabels = list(docs['token'])\n",
    "data = list(docs['TokenizedText'])\n",
    "sentences = TaggedDocumentIterator(data, docLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d300,n5,mc2,t32) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d300,n5,w10,mc2,t32) vocabulary scanned & state initialized\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "# This gives use two models one for DM and other for DBOW\n",
    "%time simple_models = prep2.doc_model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next two steps is training the models\n",
    "# This is DBOW\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "print(\"Training %s\" % simple_models[0])\n",
    "simple_models[0].train(sentences, total_examples=simple_models[0].corpus_count , epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is DM\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "print(\"Training %s\" % simple_models[1])\n",
    "simple_models[1].train(sentences, total_examples=simple_models[1].corpus_count , epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models[0].save('DBOW.doc2vec')\n",
    "simple_models[1].save('DM.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
